{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/launchpad/miniforge3/envs/argus/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./utils/data/Scraped News/marketinsights_data_temp.csv']\n",
      "./utils/data/Scraped News/marketinsights_data_temp.csv\n",
      "Labelling Country and Industry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pipeline issue to be resolved...\n",
    "Exact same code here but it works\n",
    "Run every week if needed\n",
    "\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "from utils.marketinsights import process_tables, get_city_mapping, get_contact_information, get_embeddings, get_industry, get_intersection, get_phone_mapping, label_country_by_city, label_country_by_phone, final_country, convert_to_datetime, AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = 'nomic-ai/nomic-embed-text-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "selection = 'marketinsights'\n",
    "directory = \"./utils/data/Scraped News/\"\n",
    "file_pattern = f\"{selection}_data_*.csv\"\n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "print(files)\n",
    "latest_file = max(files, key=os.path.getctime)\n",
    "print(latest_file)\n",
    "\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "df_temp = df['tables'].apply(lambda x: pd.Series(process_tables(x)))\n",
    "df_temp.columns = ['Executives', 'Shareholders']\n",
    "df[['Executives', 'Shareholders']] = df_temp\n",
    "\n",
    "print('Labelling Country and Industry')\n",
    "\n",
    "try:\n",
    "    existing = pd.read_csv('utils/data/Scrape/phoneextensions.csv')\n",
    "except:\n",
    "    existing = pd.DataFrame()\n",
    "phone_storage = get_phone_mapping(existing)\n",
    "city_storage = get_city_mapping()\n",
    "df['Industry'] = df['raw'].apply(get_industry)\n",
    "df['Contact Information'] = df['raw'].apply(get_contact_information)\n",
    "df['Country_phone'] = df['Contact Information'].apply(lambda x: label_country_by_phone(x, phone_storage))\n",
    "df['Country_city'] = df['Contact Information'].apply(lambda x: label_country_by_city(x, city_storage))\n",
    "df['Country_candidates'] = df.apply(lambda x: get_intersection(x.Country_phone, x.Country_city), axis = 1)\n",
    "df['Country'] = df.apply(lambda x: final_country(x['Contact Information'], x.Country_candidates, tokenizer, model), axis = 1)\n",
    "\n",
    "# Apply the conversion functions\n",
    "df['Time'] = df['date'].apply(convert_to_datetime)\n",
    "df.drop(['date'], axis = 1)\n",
    "df.to_csv('utils/data/Scraped News/marketinsights_data_2024-07-11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trying out sentiment scoring for article content\n",
    "'''\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "test_content = df.iloc[5]['Article content']\n",
    "print(test_content)\n",
    "\n",
    "# initialize our model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    # define our starting position (0) and window size (number of tokens in each chunk)\n",
    "    start = 0\n",
    "    window_size = 512\n",
    "    \n",
    "    # initialize probabilities list\n",
    "    probs_list = []\n",
    "    \n",
    "    start = 0\n",
    "    window_size = 510  # we take 2 off here so that we can fit in our [CLS] and [SEP] tokens\n",
    "    \n",
    "    loop = True\n",
    "    \n",
    "    while loop:\n",
    "        end = start + window_size\n",
    "        if end >= total_len:\n",
    "            loop = False\n",
    "            end = total_len\n",
    "        # (1) extract window from input_ids and attention_mask\n",
    "        input_ids_chunk = input_ids[start:end]\n",
    "        attention_mask_chunk = attention_mask[start:end]\n",
    "        # (2) add [CLS] and [SEP]\n",
    "        input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "        attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "        # (3) add padding upto window_size + 2 (512) tokens\n",
    "        input_ids_chunk += [0] * (window_size - len(input_ids_chunk) + 2)\n",
    "        attention_mask_chunk += [0] * (window_size - len(attention_mask_chunk) + 2)\n",
    "        # (4) format into PyTorch tensors dictionary\n",
    "        input_dict = {\n",
    "            'input_ids': torch.Tensor([input_ids_chunk]).long(),\n",
    "            'attention_mask': torch.Tensor([attention_mask_chunk]).int()\n",
    "        }\n",
    "        # (5) make logits prediction\n",
    "        outputs = model(**input_dict)\n",
    "        # (6) calculate softmax and append to list\n",
    "        probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "        probs_list.append(probs)\n",
    "    \n",
    "        start = end\n",
    "\n",
    "    stacks = torch.stack(probs_list)\n",
    "    shape = stacks.shape\n",
    "    with torch.no_grad():\n",
    "        # we must include our stacks operation in here too\n",
    "        stacks = torch.stack(probs_list)\n",
    "        # now resize\n",
    "        stacks = stacks.resize_(stacks.shape[0], stacks.shape[2])\n",
    "        # finally, we can calculate the mean value for each sentiment class\n",
    "        mean = stacks.mean(dim=0)\n",
    "    winner = torch.argmax(mean).item()\n",
    "    result = ['Positive', 'Negative', 'Neutral'][winner]\n",
    "    return result\n",
    "\n",
    "df['Sentiment'] = df['Article content'].apply(get_sentiment)\n",
    "for i, row in df.iterrows():\n",
    "    print('Content: \\n')\n",
    "    print(row['Article content'])\n",
    "    print('\\nSentiment: \\n')\n",
    "    print(row['Sentiment'])\n",
    "    print('**********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trying out labelling missing executives from company website\n",
    "'''\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def search_company_management(company_name, api_key):\n",
    "    # Define common terms used for management page\n",
    "    management_terms = {\n",
    "        \"Team\", \"Executives\", \"Leadership\", \n",
    "        \"Our Team\", \"Leaders\", \n",
    "        \"Directors\", \"Management\", \n",
    "        \"Executive\", \"Board\"}\n",
    "    \n",
    "    # Construct query with various combinations\n",
    "    queries = f\"{company_name} company AND {' OR '.join(management_terms)}\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": queries,\n",
    "        \"engine\": \"google\",\n",
    "        \"google_domain\": \"google.com\",\n",
    "        \"num\": 10,\n",
    "        \"api_key\": api_key,\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return results\n",
    "def extract_management_info(text):\n",
    "    # Define common position titles\n",
    "    position_titles = [\n",
    "        \"CEO\", \"Chief Executive Officer\", \"CFO\", \"Chief Financial Officer\", \"COO\", \n",
    "        \"Chief Operating Officer\", \"CTO\", \"Chief Technology Officer\", \"CMO\", \n",
    "        \"Chief Marketing Officer\", \"Manager\", \"Director\", \"Vice President\", \n",
    "        \"VP\", \"President\", \"Chairman\", \"Founder\", \"Owner\", \"Partner\", \"Head\"\n",
    "    ]\n",
    "    \n",
    "    # Create a regex pattern to find names and positions\n",
    "    # This pattern assumes that names are capitalized words (with optional middle names/initials)\n",
    "    name_pattern = r'([A-Z][a-z]+(?: [A-Z][a-z]*)*)'\n",
    "    position_pattern = r'(' + '|'.join(position_titles) + r')'\n",
    "    combined_pattern = re.compile(rf'{name_pattern}\\s*,?\\s*{position_pattern}|{position_pattern}\\s*,?\\s*{name_pattern}', re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = combined_pattern.findall(text)\n",
    "    \n",
    "    # Clean up matches\n",
    "    management_info = []\n",
    "    for match in matches:\n",
    "        # Filter out empty strings and None\n",
    "        filtered_match = [m for m in match if m]\n",
    "        if len(filtered_match) == 2:\n",
    "            if filtered_match[1] in position_titles:\n",
    "                management_info.append((filtered_match[0].strip(), filtered_match[1].strip()))\n",
    "            else:\n",
    "                management_info.append((filtered_match[1].strip(), filtered_match[0].strip()))\n",
    "    \n",
    "    return management_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = st.secrets['serp_api_key']\n",
    "company_name = 'Snapdocs'\n",
    "search_results = search_company_management(company_name, api_key)\n",
    "print(search_results)\n",
    "temp = []\n",
    "search_results.keys()\n",
    "for dictionary in search_results['organic_results']:\n",
    "    temp.append(dictionary['link'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to scrape https://ir.tesla.com/corporate\n",
      "Unable to scrape https://www.wsj.com/market-data/quotes/TSLA/company-people\n",
      "Unable to scrape https://ir.tesla.com/contact-us\n",
      "Name: CSR, Position: Partner\n",
      "Name: cto, Position: Public Se\n",
      "Name: cto, Position: Actionable se\n",
      "Name: cto, Position: Fa\n",
      "Name: executives, Position: CEO\n",
      "Name: Elon Musk, Position: Chief Executive Officer\n",
      "Name: cto, Position: Dire\n",
      "Name: Elon Musk has been the, Position: Chief Executive Officer\n",
      "Name: cto, Position: and Dire\n",
      "Name: and was, Position: Chairman\n",
      "Name: he also serves as the Chief Executive Officer, Position: Chief Technology Officer\n",
      "Name: and, Position: Chairman\n",
      "Name: of Twitter Inc since, Position: Chief Executive Officer\n",
      "Name: cto, Position: and Dire\n",
      "Name: Senior Management, Position: Chief Financial Officer\n",
      "Name: Vaibhav Taneja has been the, Position: Chief Financial Officer\n",
      "Name: Denholm, Position: Chairman\n",
      "Name: Denholm has been the, Position: Chairman\n",
      "Name: cto, Position: she served as the Chief Financial Officer and Head of Strategy of Telstra Corporation Limited from O\n",
      "Name: cto, Position: to O\n",
      "Name: Denholm also served as the Executive Vice President, Position: Chief Financial Officer\n",
      "Name: she also serves as an Operating, Position: Partner\n",
      "Name: Senior Vice, Position: President\n",
      "Name: Kimbal Musk, Position: Director\n"
     ]
    }
   ],
   "source": [
    "storage = {}\n",
    "for link in temp:\n",
    "    headerx = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "        }\n",
    "    # Send a GET request to the URL\n",
    "    try:\n",
    "        response = requests.get(link, headers = headerx)\n",
    "        response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "        # Parse the HTML content of the page with BeautifulSoup\n",
    "        html_content = urllib.parse.unquote(response.text)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Extract and clean text content\n",
    "        text_content = soup.get_text(separator=' ', strip=False)\n",
    "        \n",
    "        # Extract management information\n",
    "        management_info = extract_management_info(text_content)\n",
    "        \n",
    "        # Print the extracted management information\n",
    "        for name, position in management_info:\n",
    "            if name not in storage.keys():\n",
    "                storage[name] = {position: 1}\n",
    "            else:\n",
    "                if position not in storage[name].keys():\n",
    "                    storage[name] = {position: 1}\n",
    "                else:\n",
    "                    storage[name][position] += 1\n",
    "    except:\n",
    "        print(f\"Unable to scrape {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
