{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/launchpad/miniforge3/envs/argus/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./utils/data/Scraped News/marketinsights_data_temp.csv']\n",
      "./utils/data/Scraped News/marketinsights_data_temp.csv\n",
      "Labelling Country and Industry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pipeline issue to be resolved...\n",
    "Exact same code here but it works\n",
    "Run every week if needed\n",
    "\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "from utils.marketinsights import process_tables, get_city_mapping, get_contact_information, get_embeddings, get_industry, get_intersection, get_phone_mapping, label_country_by_city, label_country_by_phone, final_country, convert_to_datetime, AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = 'nomic-ai/nomic-embed-text-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "selection = 'marketinsights'\n",
    "directory = \"./utils/data/Scraped News/\"\n",
    "file_pattern = f\"{selection}_data_*.csv\"\n",
    "files = glob.glob(os.path.join(directory, file_pattern))\n",
    "print(files)\n",
    "latest_file = max(files, key=os.path.getctime)\n",
    "print(latest_file)\n",
    "\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "df_temp = df['tables'].apply(lambda x: pd.Series(process_tables(x)))\n",
    "df_temp.columns = ['Executives', 'Shareholders']\n",
    "df[['Executives', 'Shareholders']] = df_temp\n",
    "\n",
    "print('Labelling Country and Industry')\n",
    "\n",
    "try:\n",
    "    existing = pd.read_csv('utils/data/Scrape/phoneextensions.csv')\n",
    "except:\n",
    "    existing = pd.DataFrame()\n",
    "phone_storage = get_phone_mapping(existing)\n",
    "city_storage = get_city_mapping()\n",
    "df['Industry'] = df['raw'].apply(get_industry)\n",
    "df['Contact Information'] = df['raw'].apply(get_contact_information)\n",
    "df['Country_phone'] = df['Contact Information'].apply(lambda x: label_country_by_phone(x, phone_storage))\n",
    "df['Country_city'] = df['Contact Information'].apply(lambda x: label_country_by_city(x, city_storage))\n",
    "df['Country_candidates'] = df.apply(lambda x: get_intersection(x.Country_phone, x.Country_city), axis = 1)\n",
    "df['Country'] = df.apply(lambda x: final_country(x['Contact Information'], x.Country_candidates, tokenizer, model), axis = 1)\n",
    "\n",
    "# Apply the conversion functions\n",
    "df['Time'] = df['date'].apply(convert_to_datetime)\n",
    "df.drop(['date'], axis = 1)\n",
    "df.to_csv('utils/data/Scraped News/marketinsights_data_2024-07-11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trying out sentiment scoring for article content\n",
    "'''\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "test_content = df.iloc[5]['Article content']\n",
    "print(test_content)\n",
    "\n",
    "# initialize our model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    # define our starting position (0) and window size (number of tokens in each chunk)\n",
    "    start = 0\n",
    "    window_size = 512\n",
    "    \n",
    "    # initialize probabilities list\n",
    "    probs_list = []\n",
    "    \n",
    "    start = 0\n",
    "    window_size = 510  # we take 2 off here so that we can fit in our [CLS] and [SEP] tokens\n",
    "    \n",
    "    loop = True\n",
    "    \n",
    "    while loop:\n",
    "        end = start + window_size\n",
    "        if end >= total_len:\n",
    "            loop = False\n",
    "            end = total_len\n",
    "        # (1) extract window from input_ids and attention_mask\n",
    "        input_ids_chunk = input_ids[start:end]\n",
    "        attention_mask_chunk = attention_mask[start:end]\n",
    "        # (2) add [CLS] and [SEP]\n",
    "        input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "        attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "        # (3) add padding upto window_size + 2 (512) tokens\n",
    "        input_ids_chunk += [0] * (window_size - len(input_ids_chunk) + 2)\n",
    "        attention_mask_chunk += [0] * (window_size - len(attention_mask_chunk) + 2)\n",
    "        # (4) format into PyTorch tensors dictionary\n",
    "        input_dict = {\n",
    "            'input_ids': torch.Tensor([input_ids_chunk]).long(),\n",
    "            'attention_mask': torch.Tensor([attention_mask_chunk]).int()\n",
    "        }\n",
    "        # (5) make logits prediction\n",
    "        outputs = model(**input_dict)\n",
    "        # (6) calculate softmax and append to list\n",
    "        probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "        probs_list.append(probs)\n",
    "    \n",
    "        start = end\n",
    "\n",
    "    stacks = torch.stack(probs_list)\n",
    "    shape = stacks.shape\n",
    "    with torch.no_grad():\n",
    "        # we must include our stacks operation in here too\n",
    "        stacks = torch.stack(probs_list)\n",
    "        # now resize\n",
    "        stacks = stacks.resize_(stacks.shape[0], stacks.shape[2])\n",
    "        # finally, we can calculate the mean value for each sentiment class\n",
    "        mean = stacks.mean(dim=0)\n",
    "    winner = torch.argmax(mean).item()\n",
    "    result = ['Positive', 'Negative', 'Neutral'][winner]\n",
    "    return result\n",
    "\n",
    "df['Sentiment'] = df['Article content'].apply(get_sentiment)\n",
    "for i, row in df.iterrows():\n",
    "    print('Content: \\n')\n",
    "    print(row['Article content'])\n",
    "    print('\\nSentiment: \\n')\n",
    "    print(row['Sentiment'])\n",
    "    print('**********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trying out labelling missing executives from company website\n",
    "'''\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def search_company_management(company_name, api_key):\n",
    "    # Define common terms used for management page\n",
    "    management_terms = {\n",
    "        \"Team\", \"Leadership\", \"Leaders\", \n",
    "        \"Directors\", \"Management\", \n",
    "        \"Executive\", \"Board\", \"Organization\"}\n",
    "    \n",
    "    # Construct query with various combinations\n",
    "    queries = f\"({company_name} company website) AND ({' OR '.join(management_terms)})\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": queries,\n",
    "        \"engine\": \"google\",\n",
    "        \"google_domain\": \"google.com\",\n",
    "        \"num\": 10,\n",
    "        \"nfpr\": 1,\n",
    "        \"api_key\": api_key,\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    # Scraping each result page\n",
    "    storage = []\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "        }\n",
    "    for temp in results['organic_results']:\n",
    "        print(temp.keys())\n",
    "        link = temp['link']\n",
    "        try:\n",
    "            response = requests.get(link, timeout=10)\n",
    "            response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "        # Parse the HTML content of the page with BeautifulSoup\n",
    "            html_content = urllib.parse.unquote(response.text)\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            # Extract and clean text content\n",
    "            text_content = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            storage.append(text_content)\n",
    "        except:\n",
    "            print(f\"Unable to scrape {link}\")\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'sitelinks', 'source'])\n",
      "Unable to scrape https://ir.tesla.com/corporate\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'source'])\n",
      "Unable to scrape https://www.tesla.com/about\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'source'])\n",
      "Unable to scrape https://ir.tesla.com/corporate/elon-musk\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'source'])\n",
      "Unable to scrape https://ir.tesla.com/contact-us\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'sitelinks', 'source'])\n",
      "Unable to scrape https://www.tesla.com/elon-musk\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'sitelinks', 'source'])\n",
      "Unable to scrape https://en.wikipedia.org/wiki/Tesla,_Inc.\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'source'])\n",
      "Unable to scrape https://www.organimi.com/organizational-structures/tesla/\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'snippet', 'snippet_highlighted_words', 'sitelinks', 'rich_snippet', 'source'])\n",
      "Unable to scrape https://www.globaldata.com/company-profile/tesla-inc/\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'source'])\n",
      "Unable to scrape https://www.linkedin.com/company/tesla-motors\n",
      "dict_keys(['position', 'title', 'link', 'redirect_link', 'displayed_link', 'favicon', 'snippet', 'snippet_highlighted_words', 'sitelinks', 'source'])\n",
      "Unable to scrape https://www.tesla.com/\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = st.secrets['serp_api_key']\n",
    "company_name = 'Tesla'\n",
    "search_results = search_company_management(company_name, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in search_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Elon Musk, Position: CEO, Count: 3\n",
      "Name: is Elon Musk, Position: CEO, Count: 2\n",
      "Name: Compensation Analysis How has Elon Musk, Position: CEO, Count: 2\n",
      "Name: Musk is Executive, Position: Chairman, Count: 2\n",
      "Name: Vaibhav Taneja, Position: Chief Financial Officer, Count: 2\n",
      "Name: Xiaotong Zhu Senior Vice, Position: President, Count: 2\n",
      "Name: Martin Viecha Vice President of Investor Relations no data no data no data Brian Scelfo Senior Director of Corporate Development no data no data no data Franz von Holzhausen Chief Designer no data no data no data John Walker Vice, Position: President, Count: 2\n",
      "Name: no data Peter Bannon Chip Architect no data no data no data Turner Caldwell Engineering Manager no data no data no data Rodney Westmoreland Director of Construction Management no data no data no data Lars Moravy Vice, Position: President, Count: 2\n",
      "Name: with Musk eventually stepping down as, Position: Chairman, Count: 2\n",
      "Name: Team members Elon Musk, Position: CEO, Count: 1\n",
      "Name: CFO Zachary Kirkhorn has been serving as, Position: CFO, Count: 1\n",
      "Name: SVP Drew Baglino is the S, Position: VP, Count: 1\n",
      "Name: backed by Zachary as a, Position: CFO, Count: 1\n",
      "Name: Drew as S, Position: VP, Count: 1\n",
      "Name: Lars Moravy as, Position: VP, Count: 1\n",
      "Name: Musk served as CEO, Position: Chairman, Count: 1\n",
      "Name: Musk had previously held both the CEO and, Position: Chairman, Count: 1\n",
      "Name: Andrew Baglino Senior vice president Powertrain and engineering Vaibhav Taneja, Position: CFO, Count: 1\n",
      "Name: supply chain Lars Moravy, Position: Head, Count: 1\n",
      "Name: and security Charles Kuehmann, Position: VP, Count: 1\n",
      "Name: DBL Partners Joe Gebbia Cofounder of Airbnb James Murdoch, Position: CEO, Count: 1\n",
      "Name: ed by famed entrepreneur Elon Musk, Position: head, Count: 1\n",
      "Name: Zachary Kirkhorn is the, Position: Chief Financial Officer, Count: 1\n",
      "Name: Andrew Baglino is the Senior Vice, Position: President, Count: 1\n",
      "Name: Kirkhorn Having assumed the position of, Position: Chief Financial Officer, Count: 1\n",
      "Name: Kirkhorn succeeded Deepak Ahuja as, Position: CFO, Count: 1\n",
      "Name: Zach was most recently Vice, Position: President, Count: 1\n",
      "Name: Deepak Ahuja Deepak Ahuja was, Position: CFO, Count: 1\n",
      "Name: He is the younger son of media mogul Rupert Murdoch and former, Position: CEO, Count: 1\n",
      "Name: Kimbal Musk is also the founder and, Position: CEO, Count: 1\n",
      "Name: Independent Director Kimbal Musk Director Robyn Denholm, Position: Chairman, Count: 1\n",
      "Name: Executives Elon Musk, Position: Chief Executive Officer, Count: 1\n",
      "Name: Elon Musk has been the, Position: Chief Executive Officer, Count: 1\n",
      "Name: Vaibhav Taneja has been the, Position: Chief Financial Officer, Count: 1\n",
      "Name: Denholm, Position: Chairman, Count: 1\n",
      "Name: Denholm has been the, Position: Chairman, Count: 1\n",
      "Name: Denholm also served as the Executive Vice President, Position: Chief Financial Officer, Count: 1\n",
      "Name: Denholm Tom Zhu Senior Vice, Position: President, Count: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Initialize BERT NER pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_management_info(text, nlp):\n",
    "    # Define common position titles\n",
    "    position_titles = [\n",
    "        \"CEO\", \"Chief Executive Officer\", \"CFO\", \"Chief Financial Officer\", \"COO\", \n",
    "        \"Chief Operating Officer\", \"CTO\", \"Chief Technology Officer\", \"CMO\", \n",
    "        \"Chief Marketing Officer\", \"Director\", \"Vice President\", \n",
    "        \"VP\", \"President\", \"Chairman\", \"Founder\", \"Owner\", \"Partner\", \"Head\"\n",
    "    ]\n",
    "    \n",
    "    # Create a regex pattern to find names and positions\n",
    "    name_pattern = r'([A-Z][a-z]+(?: [A-Z][a-z]*)*)'\n",
    "    position_pattern = r'(' + '|'.join(position_titles) + r')'\n",
    "    combined_pattern = re.compile(rf'{name_pattern}\\s*,?\\s*{position_pattern}|{position_pattern}\\s*,?\\s*{name_pattern}', re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = combined_pattern.findall(text)\n",
    "    \n",
    "    # Extract names and positions\n",
    "    management_info = []\n",
    "    for match in matches:\n",
    "        filtered_match = [m for m in match if m]\n",
    "        if len(filtered_match) == 2:\n",
    "            name, position = (filtered_match[0].strip(), filtered_match[1].strip())\n",
    "            if position in position_titles:\n",
    "                # Validate the name using BERT NER\n",
    "                ner_results = nlp(name)\n",
    "                if any('PER' in item['entity'] and item['score'] >= 0.95 for item in ner_results):\n",
    "                    management_info.append((item['entity'], position))\n",
    "            else:\n",
    "                name, position = (filtered_match[1].strip(), filtered_match[0].strip())\n",
    "                # Validate the name using BERT NER\n",
    "                ner_results = nlp(name)\n",
    "                if any('PER' in item['entity'] and item['score'] >= 0.95 for item in ner_results):\n",
    "                    management_info.append((name, position))\n",
    "\n",
    "    return management_info\n",
    "\n",
    "# Process each entry in the storage\n",
    "name_position_pairs = []\n",
    "for key in storage:\n",
    "    text = key\n",
    "    management_info = extract_management_info(text, nlp)\n",
    "    name_position_pairs.extend(management_info)\n",
    "\n",
    "# Count occurrences of each name-position pair\n",
    "pair_counter = Counter(name_position_pairs)\n",
    "\n",
    "# Find the most common pairs\n",
    "most_common_pairs = pair_counter.most_common()\n",
    "\n",
    "# Print the most common name-position pairs\n",
    "for pair, count in most_common_pairs:\n",
    "    print(f\"Name: {pair[0]}, Position: {pair[1]}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
